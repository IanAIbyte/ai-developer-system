"""
LLM Clients - å¤§è¯­è¨€æ¨¡å‹å®¢æˆ·ç«¯é›†æˆ

æ”¯æŒå¤šä¸ª LLM æä¾›å•†ï¼š
- GLM-5 (æ™ºè°±AI)
- Claude (Anthropic)
- OpenAI (å¯é€‰ï¼‰
"""

import os
import json
import httpx
from typing import Dict, List, Optional, Any
from pathlib import Path

# è‡ªåŠ¨åŠ è½½ .env æ–‡ä»¶
try:
    from dotenv import load_dotenv
    # å°è¯•ä»å½“å‰ç›®å½•å’Œå·¥ä½œç›®å½•åŠ è½½ .env
    load_dotenv()
    load_dotenv(os.path.join(os.path.dirname(__file__), '..', '.env'))
except ImportError:
    pass  # å¦‚æœæ²¡æœ‰å®‰è£… python-dotenvï¼Œå¿½ç•¥


class GLM5Client:
    """
    GLM API å®¢æˆ·ç«¯ï¼ˆæ”¯æŒ GLM Coding Plan Pro/Max å¥—é¤ï¼‰

    æ–‡æ¡£: https://docs.bigmodel.cn/cn/api/introduction
    å¥—é¤: https://www.bigmodel.cn/glm-coding

    æ”¯æŒçš„å¥—é¤å’Œæ¨¡å‹ï¼ˆæ›´æ–°äº 2026-02-14ï¼‰ï¼š
    - Pro å¥—é¤ï¼šglm-5ï¼ˆæ¨èï¼‰ã€glm-4.7ã€glm-4.6ã€glm-4.5ã€glm-4.5-air
    - Max å¥—é¤ï¼šglm-5ï¼ˆæ¨èï¼‰ã€glm-4.7 ç­‰
    - Lite å¥—é¤ï¼šglm-4.7 ç­‰ï¼ˆglm-5 æ”¯æŒå³å°†ä¸Šçº¿ï¼‰

    ç«¯ç‚¹è¯´æ˜ï¼š
    - é€šç”¨ API: https://open.bigmodel.cn/api/paas/v4 (å¯¹è¯ã€åˆ†æç­‰)
    - Coding API: https://open.bigmodel.cn/api/coding/paas/v4 (ä»…ä»£ç ç”Ÿæˆ)
    """

    # é€šç”¨ API ç«¯ç‚¹ï¼ˆç”¨äºå¯¹è¯ã€éœ€æ±‚åˆ†æç­‰ï¼‰
    API_GENERAL = "https://open.bigmodel.cn/api/paas/v4/chat/completions"

    # Coding API ç«¯ç‚¹ï¼ˆä»…ç”¨äºä»£ç ç”Ÿæˆï¼‰
    API_CODING = "https://open.bigmodel.cn/api/coding/paas/v4/chat/completions"

    # æ ¹æ®å¥—é¤é€‰æ‹©åˆé€‚çš„æ¨¡å‹ï¼ˆæ›´æ–°äº 2026-02-14ï¼‰
    # Pro/Max å¥—é¤æ¨èä½¿ç”¨ glm-5ï¼ˆæœ€æ–°æ——èˆ°æ¨¡å‹ï¼Œç¼–ç¨‹ä½“æ„Ÿé€¼è¿‘ Claude 4.5/4.6ï¼‰
    # Pro/Max å¥—é¤ä¹Ÿå¯ä»¥ä½¿ç”¨ glm-4.7ã€glm-4.6ã€glm-4.5
    MODEL_NAME = "glm-5"  # é»˜è®¤ä½¿ç”¨ GLM-5ï¼ˆä¸å®˜æ–¹ç¤ºä¾‹ä¸€è‡´ï¼‰

    # å¯é€‰æ¨¡å‹åˆ—è¡¨ï¼ˆæ›´æ–°äº 2026-02-14ï¼‰
    AVAILABLE_MODELS = {
        "glm-5": "GLM-5ï¼ˆæ¨èï¼‰- Pro/Max å¥—é¤ï¼Œæœ€æ–°æ——èˆ°æ¨¡å‹",
        "glm-4.7": "GLM-4.7 - Pro/Max å¥—é¤ï¼Œä»£ç ç”Ÿæˆä¼˜åŒ–",
        "glm-4.6": "GLM-4.6 - Pro/Max å¥—é¤",
        "glm-4.5": "GLM-4.5 - Pro/Max å¥—é¤",
        "glm-4.5-air": "GLM-4.5-Air - è½»é‡çº§ï¼Œæ‰€æœ‰å¥—é¤"
    }

    def __init__(self, api_key: Optional[str] = None):
        """
        åˆå§‹åŒ– GLM-5 å®¢æˆ·ç«¯

        Args:
            api_key: æ™ºè°±AI API Key (å¯ä»¥ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
        """
        self.api_key = api_key or os.getenv("ZHIPUAI_API_KEY")
        if not self.api_key:
            raise ValueError(
                "API Key not found. Please set ZHIPUAI_API_KEY "
                "environment variable or pass api_key parameter."
            )

        # ä»ç¯å¢ƒå˜é‡è¯»å–è¶…æ—¶é…ç½®
        timeout_general = float(os.getenv("GLM5_TIMEOUT", "90"))
        timeout_coding = float(os.getenv("GLM5_CODING_TIMEOUT", "120"))

        # é€šç”¨ API å®¢æˆ·ç«¯ï¼ˆç”¨äºå¯¹è¯ã€éœ€æ±‚åˆ†æç­‰ï¼‰
        self.client_general = httpx.Client(
            base_url=self.API_GENERAL,
            headers={
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            },
            timeout=timeout_general
        )

        # Coding API å®¢æˆ·ç«¯ï¼ˆä»…ç”¨äºä»£ç ç”Ÿæˆï¼‰
        self.client_coding = httpx.Client(
            base_url=self.API_CODING,
            headers={
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            },
            timeout=timeout_coding
        )

    def chat_completion(
            self,
            messages: List[Dict],
            tools: Optional[List[Dict]] = None,
            temperature: float = 0.7,
            max_tokens: int = 4096,
            stream: bool = False,
            show_progress: bool = False
    ) -> Dict:
        """
        è°ƒç”¨ GLM é€šç”¨å¯¹è¯è¡¥å…¨ API

        ç”¨äºï¼šéœ€æ±‚åˆ†æã€å¯¹è¯äº¤äº’ç­‰é€šç”¨åœºæ™¯

        Args:
            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
                [{"role": "user", "content": "..."}]
            tools: å·¥å…·åˆ—è¡¨ï¼ˆFunction Callingï¼‰
            temperature: æ¸©åº¦å‚æ•° (0-1)
            max_tokens: æœ€å¤§ token æ•°
            stream: æ˜¯å¦æµå¼è¾“å‡º
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦ä¿¡æ¯

        Returns:
            API å“åº”å­—å…¸
        """
        payload = {
            "model": self.MODEL_NAME,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "stream": stream
        }

        if tools:
            payload["tools"] = tools

        try:
            if show_progress:
                import sys
                import time
                print("[GLM-5] Sending request to API...", file=sys.stderr, flush=True)
                start_time = time.time()

            # ä½¿ç”¨é€šç”¨ API ç«¯ç‚¹
            response = self.client_general.post("", json=payload)
            response.raise_for_status()

            if show_progress:
                elapsed = time.time() - start_time
                print(f"[GLM-5] Response received in {elapsed:.1f}s", file=sys.stderr, flush=True)

            if stream:
                # æµå¼å“åº”
                return {
                    "stream": response.iter_bytes()
                }

            return response.json()

        except httpx.HTTPStatusError as e:
            return {
                "error": True,
                "status_code": e.response.status_code,
                "message": str(e)
            }
        except Exception as e:
            return {
                "error": True,
                "message": str(e)
            }

    def coding_completion(
            self,
            messages: List[Dict],
            temperature: float = 0.3,
            max_tokens: int = 8192,
            stream: bool = False
    ) -> Dict:
        """
        è°ƒç”¨ GLM Coding APIï¼ˆä¸“ç”¨ç«¯ç‚¹ï¼‰

        ç”¨äºï¼šä»£ç ç”Ÿæˆã€ä»£ç ä¼˜åŒ–ç­‰ Coding åœºæ™¯

        æ³¨æ„ï¼šæ­¤ç«¯ç‚¹ä»…é™ Coding åœºæ™¯ä½¿ç”¨ï¼Œä¸é€‚ç”¨äºé€šç”¨å¯¹è¯

        Args:
            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
                [{"role": "user", "content": "..."}]
            temperature: æ¸©åº¦å‚æ•° (0-1)
            max_tokens: æœ€å¤§ token æ•°
            stream: æ˜¯å¦æµå¼è¾“å‡º

        Returns:
            API å“åº”å­—å…¸
        """
        payload = {
            "model": self.MODEL_NAME,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "stream": stream
        }

        try:
            # ä½¿ç”¨ Coding API ç«¯ç‚¹
            response = self.client_coding.post("", json=payload)
            response.raise_for_status()

            if stream:
                # æµå¼å“åº”
                return {
                    "stream": response.iter_bytes()
                }

            return response.json()

        except httpx.HTTPStatusError as e:
            return {
                "error": True,
                "status_code": e.response.status_code,
                "message": str(e)
            }
        except Exception as e:
            return {
                "error": True,
                "message": str(e)
            }

    def generate_code(
            self,
            prompt: str,
            context: Optional[str] = None,
            file_structure: Optional[Dict] = None,
            temperature: float = 0.3,
            max_tokens: int = 8192
    ) -> str:
        """
        ç”Ÿæˆä»£ç ï¼ˆä¸“ç”¨æ–¹æ³•ï¼‰

        Args:
            prompt: ä»£ç ç”Ÿæˆæç¤º
            context: é¢å¤–çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
            file_structure: é¡¹ç›®æ–‡ä»¶ç»“æ„

        Returns:
            ç”Ÿæˆçš„ä»£ç å­—ç¬¦ä¸²
        """
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è½¯ä»¶å¼€å‘ AI åŠ©æ‰‹ï¼Œæ“…é•¿ä»¥ä¸‹ä»»åŠ¡ï¼š

1. åˆ†æä»£ç éœ€æ±‚å¹¶ç”Ÿæˆé«˜è´¨é‡ä»£ç 
2. ç†è§£ç°æœ‰ä»£ç ç»“æ„å¹¶è¿›è¡Œå¢é‡ä¿®æ”¹
3. éµå¾ªæœ€ä½³å®è·µå’Œè®¾è®¡æ¨¡å¼
4. ç¡®ä¿ä»£ç çš„å¯ç»´æŠ¤æ€§å’Œå¯æ‰©å±•æ€§

é‡è¦åŸåˆ™ï¼š
- ä¼˜å…ˆä½¿ç”¨ä¸å¯å˜ï¼ˆimmutableï¼‰æ•°æ®ç»“æ„
- å‡½æ•°ä¿æŒå°è€Œä¸“æ³¨ï¼ˆ<50 è¡Œï¼‰
- æ–‡ä»¶ä¿æŒèšç„¦ï¼ˆ<800 è¡Œï¼‰
- å®Œæ•´çš„é”™è¯¯å¤„ç†
- ç³»ç»Ÿè¾¹ç•ŒéªŒè¯è¾“å…¥
- æ¸…æ™°çš„å‘½åå’Œæ³¨é‡Š
"""

        messages = [
            {"role": "system", "content": system_prompt}
        ]

        # æ„å»ºå®Œæ•´æç¤º
        full_prompt = prompt

        if context:
            full_prompt = f"""
## é¡¹ç›®ä¸Šä¸‹æ–‡

{context}

## ä»»åŠ¡

{prompt}
"""

        if file_structure:
            full_prompt += f"""

## å½“å‰é¡¹ç›®ç»“æ„

```json
{json.dumps(file_structure, indent=2, ensure_ascii=False)}
```
"""

        messages.append({"role": "user", "content": full_prompt})

        # ä½¿ç”¨ Coding API ç«¯ç‚¹è¿›è¡Œä»£ç ç”Ÿæˆ
        response = self.coding_completion(
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens
        )

        if response.get("error"):
            raise Exception(f"API Error: {response['message']}")

        # æå–ç”Ÿæˆçš„å†…å®¹
        message = response["choices"][0]["message"]
        content = message.get("content", "")

        # å¦‚æœ content ä¸ºç©ºï¼Œæ£€æŸ¥ reasoning_contentï¼ˆGLM-4.7 æ¨ç†æ¨¡å‹ï¼‰
        if not content and "reasoning_content" in message:
            content = message["reasoning_content"]

        return content

    def analyze_requirements(
            self,
            user_prompt: str,
            max_features: int = 30,
            show_progress: bool = True
    ) -> List[Dict]:
        """
        åˆ†æç”¨æˆ·éœ€æ±‚ï¼Œç”ŸæˆåŠŸèƒ½åˆ—è¡¨

        Args:
            user_prompt: ç”¨æˆ·çš„åŸå§‹éœ€æ±‚æè¿°
            max_features: æœ€å¤§åŠŸèƒ½æ•°é‡ï¼ˆé»˜è®¤ 30ï¼‰
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦ä¿¡æ¯

        Returns:
            åŠŸèƒ½åˆ—è¡¨ï¼Œæ¯ä¸ªåŠŸèƒ½åŒ…å«ï¼š
            {
                "id": "category-action-001",
                "category": "authentication",
                "priority": "critical/high/medium/low",
                "description": "User can ...",
                "e2e_steps": ["step 1", "step 2", ...],
                "verification_step": "éªŒè¯é¡µé¢è·³è½¬åˆ° /dashboard ä¸”æ˜¾ç¤ºç”¨æˆ·å",
                "passes": false,
                "dependencies": []
            }
        """
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„äº§å“ç»ç†å’ŒæŠ€æœ¯æ¶æ„å¸ˆ AIï¼Œæ“…é•¿æ·±åº¦é€»è¾‘æ¨ç†ã€‚

ä»»åŠ¡ï¼šå°†ç”¨æˆ·çš„éœ€æ±‚åˆ†è§£ä¸ºç»†ç²’åº¦çš„ã€å¯æµ‹è¯•çš„åŠŸèƒ½ã€‚

## ğŸ¯ æ ¸å¿ƒåŸåˆ™ï¼šé€»è¾‘æ·±åº¦æ¨ç†

**ä¸¥ç¦**åªæè¿°è¡¨é¢è¡Œä¸ºï¼ä½ å¿…é¡»æ¨ç†å‡ºï¼š
1. **æ ¸å¿ƒç®—æ³•é€»è¾‘**ï¼šä¸åªæ˜¯"åˆ›å»ºè¾“å…¥æ¡†"ï¼Œè€Œæ˜¯"æ•°æ®å¦‚ä½•æµåŠ¨ã€å¤„ç†ã€éªŒè¯"
2. **éç›´è§‚ä¾èµ–**ï¼šéœ€è¦ä»€ä¹ˆéšè—çš„æ¡ä»¶åˆ¤æ–­ã€çŠ¶æ€ç®¡ç†ã€é”™è¯¯å¤„ç†
3. **ç¦æ­¢ç®€åŒ–å®ç°**ï¼šæ˜ç¡®ç¦æ­¢å“ªäº›å·æ‡’çš„å®ç°æ–¹å¼
4. **çœŸå®ä¸šåŠ¡é€»è¾‘**ï¼šè€ƒè™‘å®é™…ä½¿ç”¨åœºæ™¯ä¸­çš„å¤æ‚æƒ…å†µ

## âŒ é”™è¯¯ç¤ºä¾‹ï¼ˆè¡¨é¢æè¿°ï¼‰ï¼š
{
  "id": "ui-input-001",
  "description": "åˆ›å»ºè¾“å…¥æ¡†ç»„ä»¶",  // âŒ å¤ªè¡¨é¢ï¼
  "e2e_steps": ["åˆ›å»ºè¾“å…¥æ¡†", "æ˜¾ç¤ºè¾“å…¥æ¡†"]  // âŒ æ²¡æœ‰é€»è¾‘ï¼
}

## âœ… æ­£ç¡®ç¤ºä¾‹ï¼ˆæ·±åº¦æ¨ç†ï¼‰ï¼š
{
  "id": "api-optimize-001",
  "description": "å®ç°æç¤ºè¯ä¼˜åŒ– API ç«¯ç‚¹",
  "logical_requirements": {
    "must_call_llm": "å¿…é¡»è°ƒç”¨ LLM APIï¼ˆGLM-5 æˆ– Claudeï¼‰è¿›è¡Œæ™ºèƒ½ä¼˜åŒ–",
    "forbidden_patterns": ["ç¦æ­¢ç®€å•å­—ç¬¦ä¸²æ‹¼æ¥", "ç¦æ­¢åªè¿”å›æ¨¡æ¿"],
    "data_flow": "å‰ç«¯å‘é€ CO-STAR æ•°æ® â†’ åç«¯æ„å»ºæç¤ºè¯ â†’ è°ƒç”¨ LLM API â†’ è¿”å›ä¼˜åŒ–ç»“æœ",
    "error_handling": "API è¶…æ—¶å¿…é¡»æœ‰é‡è¯•æœºåˆ¶ï¼ˆ3æ¬¡ï¼‰ï¼Œå¤±è´¥æ—¶è¿”å›æ™ºèƒ½è§„åˆ™çš„ä¼˜åŒ–ç»“æœ",
    "complexity_level": "high",
    "integration_points": ["åç«¯ API é›†æˆ", "LLM API è°ƒç”¨", "é”™è¯¯å¤„ç†", "åŠ è½½çŠ¶æ€"]
  },
  "e2e_steps": [
    "è¾“å…¥æµ‹è¯•æ•°æ®ï¼ˆContext, Objectiveç­‰å­—æ®µï¼‰",
    "ç‚¹å‡»ä¼˜åŒ–æŒ‰é’®",
    "ç­‰å¾… API å“åº”ï¼ˆæ˜¾ç¤ºåŠ è½½çŠ¶æ€ï¼‰",
    "éªŒè¯è¿”å›çš„æ˜¯ä¼˜åŒ–åçš„æç¤ºè¯ï¼ˆä¸æ˜¯ç®€å•æ‹¼æ¥ï¼‰",
    "éªŒè¯åŒ…å«æ”¹è¿›å»ºè®®å’Œè´¨é‡è¯„åˆ†"
  ]
}

## æ·±åº¦æ¨ç†æ£€æŸ¥æ¸…å•

å¯¹æ¯ä¸ªåŠŸèƒ½ï¼Œé—®è‡ªå·±ï¼š
- âœ… æ˜¯å¦è¯´æ˜äº†**æ•°æ®å¦‚ä½•æµåŠ¨**ï¼Ÿ
- âœ… æ˜¯å¦è¯´æ˜äº†**å¦‚ä½•éªŒè¯é€»è¾‘æ­£ç¡®æ€§**ï¼Ÿ
- âœ… æ˜¯å¦è€ƒè™‘äº†**é”™è¯¯æƒ…å†µ**ï¼ˆAPI å¤±è´¥ã€ç½‘ç»œè¶…æ—¶ã€ç©ºè¾“å…¥ï¼‰ï¼Ÿ
- âœ… æ˜¯å¦è¯´æ˜äº†**çŠ¶æ€ç®¡ç†**ï¼ˆåŠ è½½ä¸­ã€æˆåŠŸã€å¤±è´¥ï¼‰ï¼Ÿ
- âœ… æ˜¯å¦**æ˜ç¡®ç¦æ­¢äº†ç®€åŒ–å®ç°**ï¼Ÿ

æ¯ä¸ªåŠŸèƒ½åº”è¯¥ï¼š
1. èšç„¦äºå•ä¸ªç”¨æˆ·è¡Œä¸ºæˆ–ç³»ç»ŸåŠŸèƒ½ï¼ˆå¼€å‘æ—¶é—´ä¸è¶…è¿‡ 15 åˆ†é’Ÿï¼‰
2. åŒ…å«æ¸…æ™°çš„ verification_stepï¼ˆéªŒè¯æ­¥éª¤ï¼‰
3. åŒ…å« logical_requirementsï¼ˆé€»è¾‘éœ€æ±‚ï¼‰
4. æœ‰æ˜ç¡®çš„ä¼˜å…ˆçº§
5. æ ‡æ³¨ä¾èµ–å…³ç³»

åŠŸèƒ½ ID æ ¼å¼ï¼šcategory-action-001 (ä¾‹å¦‚ï¼šauth-login-001)

åˆ†ç±»ï¼š
- setup: é¡¹ç›®è®¾ç½®å’Œé…ç½®
- ui: ç”¨æˆ·ç•Œé¢ç»„ä»¶
- data: æ•°æ®ç»“æ„å’ŒçŠ¶æ€ç®¡ç†
- api: API ç«¯ç‚¹å’Œä¸šåŠ¡é€»è¾‘
- testing: æµ‹è¯•ç›¸å…³
- style: æ ·å¼å’Œå¸ƒå±€

ä¼˜å…ˆçº§ï¼š
- critical: æ ¸å¿ƒåŠŸèƒ½ï¼Œæ²¡æœ‰å®ƒåº”ç”¨æ— æ³•ä½¿ç”¨
- high: é‡è¦åŠŸèƒ½
- medium: å¸¸è§„åŠŸèƒ½
- low: å¢å¼ºåŠŸèƒ½

è¾“å‡ºæ ¼å¼ï¼šJSON æ•°ç»„ï¼Œæ¯ä¸ªåŠŸèƒ½åŒ…å«ï¼š
{
  "id": "category-action-001",
  "category": "authentication",
  "priority": "critical",
  "description": "ç”¨æˆ·å¯ä»¥ç™»å½•ç³»ç»Ÿ",
  "logical_requirements": {
    "data_flow": "æ•°æ®å¦‚ä½•æµåŠ¨å’Œå¤„ç†",
    "forbidden_patterns": ["ç¦æ­¢çš„ç®€åŒ–å®ç°"],
    "error_handling": "é”™è¯¯å¤„ç†è¦æ±‚",
    "complexity_level": "low/medium/high",
    "integration_points": ["éœ€è¦é›†æˆçš„å…¶ä»–æ¨¡å—"]
  },
  "e2e_steps": ["æ­¥éª¤1", "æ­¥éª¤2", ...],
  "verification_step": "éªŒè¯é¡µé¢è·³è½¬åˆ° /dashboard ä¸”æ˜¾ç¤ºç”¨æˆ·å",
  "dependencies": [],
  "passes": false
}
"""

        messages = [
            {"role": "system", "content": system_prompt},
            {
                "role": "user",
                "content": f"""è¯·å°†ä»¥ä¸‹ç”¨æˆ·éœ€æ±‚åˆ†è§£ä¸ºè¯¦ç»†çš„åŠŸèƒ½åˆ—è¡¨ï¼š

{user_prompt}

è¦æ±‚ï¼š
1. ç”Ÿæˆ {max_features} ä¸ªå·¦å³çš„åŠŸèƒ½ï¼ˆä¸å¿…è¶…è¿‡ï¼‰
2. æ¯ä¸ªåŠŸèƒ½å¿…é¡»åŒ…å« e2e_stepsï¼ˆç«¯åˆ°ç«¯æµ‹è¯•æ­¥éª¤ï¼‰å’Œ verification_stepï¼ˆå…·ä½“éªŒè¯æ­¥éª¤ï¼‰
3. verification_step å¿…é¡»æ˜ç¡®ã€å¯æ‰§è¡Œã€å¯éªŒè¯ï¼ˆåŒ…å«å…·ä½“çš„é¢„æœŸç»“æœï¼‰
4. å¦‚æœæ¶‰åŠ UIï¼Œè¯·æ³¨æ˜å…·ä½“çš„è§†è§‰éªŒè¯è¦æ±‚ï¼ˆå¦‚é¢œè‰²ã€ä½ç½®ã€å°ºå¯¸ï¼‰
5. ä½¿ç”¨ JSON æ ¼å¼è¾“å‡º
6. åŠŸèƒ½ ID æŒ‰åˆ†ç±»å’Œç¼–å·å‘½å
7. ç¡®ä¿ä¾èµ–å…³ç³»å‡†ç¡®æ— è¯¯"""
            }
        ]

        response = self.chat_completion(
            messages=messages,
            temperature=0.5,
            max_tokens=8192,  # å‡å°‘ token æ•°é‡ï¼Œé¿å…è¶…æ—¶
            show_progress=show_progress
        )

        if response.get("error"):
            raise Exception(f"API Error: {response['message']}")

        # æå– JSON å†…å®¹
        message = response["choices"][0]["message"]
        content = message.get("content", "")

        # å¦‚æœ content ä¸ºç©ºï¼Œæ£€æŸ¥ reasoning_contentï¼ˆGLM-4.7 æ¨ç†æ¨¡å‹ï¼‰
        if not content and "reasoning_content" in message:
            content = message["reasoning_content"]

        # å°è¯•è§£æ JSONï¼ˆå¯èƒ½åŒ…è£¹åœ¨ markdown ä»£ç å—ä¸­ï¼‰
        try:
            # ç›´æ¥è§£æ
            features = json.loads(content)
        except json.JSONDecodeError:
            # å°è¯•æå– markdown ä»£ç å—
            import re
            json_match = re.search(r'```json\s*([\s\S]*?)\s*```', content)
            if json_match:
                features = json.loads(json_match.group(1))
            else:
                # å°è¯•æå–æ™®é€šä»£ç å—
                code_match = re.search(r'```\s*([\s\S]*?)\s*```', content)
                if code_match:
                    features = json.loads(code_match.group(1))
                else:
                    raise ValueError("æ— æ³•ä»å“åº”ä¸­æå–æœ‰æ•ˆçš„ JSON")

        return features


class ClaudeClient:
    """
    Claude API å®¢æˆ·ç«¯ï¼ˆå¤‡é€‰ï¼‰

    å¦‚æœç”¨æˆ·æœ‰ Anthropic API Keyï¼Œå¯ä»¥ä½¿ç”¨ Claude
    """

    API_BASE = "https://api.anthropic.com/v1/messages"

    def __init__(self, api_key: Optional[str] = None):
        """åˆå§‹åŒ– Claude å®¢æˆ·ç«¯"""
        self.api_key = api_key or os.getenv("ANTHROPIC_API_KEY")
        if not self.api_key:
            raise ValueError("ANTHROPIC_API_KEY not found")

        self.client = httpx.Client(
            base_url=self.API_BASE,
            headers={
                "x-api-key": self.api_key,
                "anthropic-version": "2023-06-01",
                "Content-Type": "application/json"
            },
            timeout=120.0
        )

    def chat_completion(self, messages: List[Dict], **kwargs) -> Dict:
        """è°ƒç”¨ Claude API"""
        # ç±»ä¼¼ GLM-5 çš„å®ç°
        payload = {
            "model": "claude-sonnet-4-5-20250929",
            "max_tokens": 4096,
            "messages": messages,
            **kwargs
        }

        response = self.client.post("/v1/messages", json=payload)
        return response.json()


def get_llm_client(provider: str = "glm-5", **kwargs) -> Any:
    """
    å·¥å‚å‡½æ•°ï¼šæ ¹æ®æä¾›å•†è¿”å›å®¢æˆ·ç«¯

    Args:
        provider: "glm-5" æˆ– "claude"
        **kwargs: ä¼ é€’ç»™å®¢æˆ·ç«¯æ„é€ å‡½æ•°çš„å‚æ•°

    Returns:
        LLM å®¢æˆ·ç«¯å®ä¾‹
    """
    providers = {
        "glm-5": GLM5Client,
        "claude": ClaudeClient
    }

    client_class = providers.get(provider.lower())
    if not client_class:
        raise ValueError(f"Unknown provider: {provider}. Available: {list(providers.keys())}")

    return client_class(**kwargs)
