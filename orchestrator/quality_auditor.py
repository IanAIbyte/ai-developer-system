"""
Quality Auditor - LLM-as-a-Judge è´¨é‡å®¡è®¡å™¨

èŒè´£ï¼š
1. ä½¿ç”¨ LLM å®¡è®¡ä»£ç è´¨é‡
2. è¯„ä¼°åŠŸèƒ½å®ç°çš„çœŸå®æ€§ï¼ˆä¸æ˜¯è¡¨é¢å·¥ä½œï¼‰
3. åˆ¤æ–­æ˜¯å¦å¯ä»¥é€šè¿‡è´¨é‡é—¨ç¦
4. æä¾›æ”¹è¿›å»ºè®®

åŸºäº Gemini Pro 3 çš„å»ºè®®ï¼š
- å‡çº§éªŒè¯æœºåˆ¶ï¼šä»"å¯ç”¨æ€§"åˆ°"è´¨é‡æ„Ÿå®˜"
- LLM-as-a-Judge éªŒè¯å™¨
- åŒé‡éªŒè¯é€»è¾‘
"""

import os
import json
import httpx
from pathlib import Path
from typing import Dict, List, Optional


class QualityAuditor:
    """è´¨é‡å®¡è®¡å™¨ - ä½¿ç”¨ LLM åˆ¤æ–­ä»£ç è´¨é‡"""

    def __init__(self):
        self.api_key = os.getenv("ZHIPUAI_API_KEY", "")
        self.api_url = "https://open.bigmodel.cn/api/paas/v4/chat/completions"

    async def audit_feature_implementation(
        self,
        feature: Dict,
        code_files: List[str],
        project_path: str
    ) -> Dict:
        """
        å®¡è®¡åŠŸèƒ½å®ç°è´¨é‡

        Args:
            feature: åŠŸèƒ½å®šä¹‰
            code_files: ä»£ç æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            project_path: é¡¹ç›®è·¯å¾„

        Returns:
            {
                "passed": bool,
                "score": int (1-10),
                "reasoning": str,
                "issues": List[str],
                "improvements": List[str]
            }
        """
        print(f"  ğŸ” [Quality Auditor] Auditing feature {feature['id']}...")

        # è¯»å–ä»£ç å†…å®¹
        code_contents = {}
        for file_path in code_files:
            full_path = Path(project_path) / file_path
            if full_path.exists():
                code_contents[file_path] = full_path.read_text()

        if not code_contents:
            return {
                "passed": False,
                "score": 1,
                "reasoning": "No code files found to audit",
                "issues": ["No implementation detected"],
                "improvements": ["Implement actual code"]
            }

        # æ„å»º audit prompt
        audit_prompt = self._build_audit_prompt(feature, code_contents)

        # è°ƒç”¨ LLM è¿›è¡Œå®¡è®¡
        try:
            result = await self._call_llm_for_audit(audit_prompt)
            return self._parse_audit_result(result)
        except Exception as e:
            print(f"  âš ï¸  LLM audit failed: {e}, using rule-based audit")
            return self._rule_based_audit(feature, code_contents)

    def _build_audit_prompt(self, feature: Dict, code_contents: Dict[str, str]) -> str:
        """æ„å»ºå®¡è®¡æç¤ºè¯"""

        # è¯»å–é€»è¾‘éœ€æ±‚ï¼ˆå¦‚æœæœ‰ï¼‰
        logical_reqs = feature.get("logical_requirements", {})

        prompt = f"""ä½ æ˜¯èµ„æ·±çš„ä»£ç è´¨é‡å®¡è®¡ä¸“å®¶ã€‚è¯·å®¡è®¡ä»¥ä¸‹åŠŸèƒ½çš„å®ç°è´¨é‡ã€‚

## åŠŸèƒ½ä¿¡æ¯
- **ID**: {feature['id']}
- **æè¿°**: {feature['description']}
- **ç±»åˆ«**: {feature.get('category', 'unknown')}
- **ä¼˜å…ˆçº§**: {feature.get('priority', 'unknown')}

## é€»è¾‘éœ€æ±‚
"""

        if logical_reqs:
            prompt += f"""
- **æ•°æ®æµ**: {logical_reqs.get('data_flow', 'Not specified')}
- **ç¦æ­¢æ¨¡å¼**: {', '.join(logical_reqs.get('forbidden_patterns', ['None']))}
- **é”™è¯¯å¤„ç†**: {logical_reqs.get('error_handling', 'Not specified')}
- **å¤æ‚åº¦**: {logical_reqs.get('complexity_level', 'unknown')}
"""
        else:
            prompt += "\n(æœªæä¾›è¯¦ç»†é€»è¾‘éœ€æ±‚)"

        prompt += f"""

## å®ç°ä»£ç 
```markdown
{self._format_code_contents(code_contents)}
```

## å®¡è®¡ç»´åº¦

è¯·ä»ä»¥ä¸‹ç»´åº¦å®¡è®¡ï¼ˆæ¯é¡¹ 1-10 åˆ†ï¼‰ï¼š

1. **é€»è¾‘çœŸå®æ€§** (1-10)
   - æ˜¯å¦å®ç°äº†çœŸå®çš„ä¸šåŠ¡é€»è¾‘ï¼ˆä¸æ˜¯è¡¨é¢å·¥ä½œï¼‰ï¼Ÿ
   - æ˜¯å¦æœ‰æ·±åº¦æ€è€ƒçš„è®¾è®¡ï¼ˆä¸æ˜¯ç®€å•æ‹¼æ¥ï¼‰ï¼Ÿ
   - æ˜¯å¦è€ƒè™‘äº†è¾¹ç•Œæƒ…å†µï¼Ÿ

2. **å®ç°å¤æ‚åº¦** (1-10)
   - ä»£ç å¤æ‚åº¦æ˜¯å¦ä¸åŠŸèƒ½åŒ¹é…ï¼Ÿ
   - æ˜¯å¦é¿å…äº†è¿‡åº¦ç®€åŒ–ï¼ˆå¦‚çº¯å­—ç¬¦ä¸²æ‹¼æ¥ï¼‰ï¼Ÿ
   - æ˜¯å¦åŒ…å«äº†å¿…è¦çš„é”™è¯¯å¤„ç†ï¼Ÿ

3. **é›†æˆå®Œæ•´æ€§** (1-10)
   - æ˜¯å¦æ­£ç¡®é›†æˆäº†æ‰€æœ‰å¿…éœ€çš„æ¨¡å—ï¼Ÿ
   - API è°ƒç”¨ã€çŠ¶æ€ç®¡ç†æ˜¯å¦æ­£ç¡®å®ç°ï¼Ÿ
   - æ˜¯å¦æœ‰åŠ è½½çŠ¶æ€ã€é”™è¯¯æç¤ºï¼Ÿ

4. **ä»£ç è´¨é‡** (1-10)
   - ä»£ç æ˜¯å¦æ¸…æ™°ã€æ˜“ç»´æŠ¤ï¼Ÿ
   - æ˜¯å¦æœ‰é€‚å½“çš„å‘½åå’Œç»“æ„ï¼Ÿ
   - æ˜¯å¦ç¬¦åˆæœ€ä½³å®è·µï¼Ÿ

5. **ç”¨æˆ·ä»·å€¼** (1-10)
   - å®ç°æ˜¯å¦çœŸæ­£è§£å†³äº†ç”¨æˆ·é—®é¢˜ï¼Ÿ
   - ç”¨æˆ·ä½“éªŒæ˜¯å¦è‰¯å¥½ï¼Ÿ

## è¾“å‡ºè¦æ±‚

è¯·ä»¥ JSON æ ¼å¼è¾“å‡ºå®¡è®¡ç»“æœï¼š
```json
{{
  "score": <æ€»ä½“è¯„åˆ† 1-10>,
  "passed": <æ˜¯å¦é€šè¿‡è´¨é‡é—¨ç¦ (score >= 7)>,
  "reasoning": "<è¯¦ç»†çš„è¯„åˆ†ç†ç”±ï¼ŒæŒ‡å‡ºä¼˜ç‚¹å’Œé—®é¢˜>",
  "dimension_scores": {{
    "logic_authenticity": <1-10>,
    "implementation_complexity": <1-10>,
    "integration_integrity": <1-10>,
    "code_quality": <1-10>,
    "user_value": <1-10>
  }},
  "issues": ["<å‘ç°çš„é—®é¢˜1>", "<å‘ç°çš„é—®é¢˜2>", ...],
  "improvements": ["<æ”¹è¿›å»ºè®®1>", "<æ”¹è¿›å»ºè®®2>", ...]
}}
```

**é‡è¦**ï¼š
- ä¸¥å‰ä½†å…¬æ­£ï¼šå¦‚æœåªæ˜¯è¡¨é¢å·¥ä½œï¼ˆå¦‚ç®€å•æ‹¼æ¥ï¼‰ï¼Œå¿…é¡»ç»™ä½åˆ†ï¼ˆ1-3åˆ†ï¼‰
- å¦‚æœé€»è¾‘éœ€æ±‚æ˜ç¡®è¦æ±‚"ç¦æ­¢ç®€å•æ‹¼æ¥"ä½†ä»£ç ä»ä½¿ç”¨æ‹¼æ¥ï¼Œç›´æ¥ä¸é€šè¿‡ï¼ˆscore < 7ï¼‰
- å¦‚æœçœ‹åˆ° TODOã€PLACEHOLDER ç­‰å ä½ç¬¦ï¼Œç›´æ¥ä¸é€šè¿‡
"""
        return prompt

    def _format_code_contents(self, code_contents: Dict[str, str]) -> str:
        """æ ¼å¼åŒ–ä»£ç å†…å®¹ç”¨äº prompt"""
        formatted = []
        for file_path, content in code_contents.items():
            formatted.append(f"### {file_path}\n")
            # é™åˆ¶æ¯ä¸ªæ–‡ä»¶æ˜¾ç¤ºçš„å­—ç¬¦æ•°
            preview = content if len(content) < 2000 else content[:2000] + "\n... (truncated)"
            formatted.append(f"```\n{preview}\n```\n")
        return "\n".join(formatted)

    async def _call_llm_for_audit(self, prompt: str) -> str:
        """è°ƒç”¨ LLM è¿›è¡Œå®¡è®¡"""
        if not self.api_key:
            raise Exception("No API key available")

        messages = [
            {
                "role": "system",
                "content": "ä½ æ˜¯ä¸€ä½èµ„æ·±çš„ä»£ç è´¨é‡å®¡è®¡ä¸“å®¶ï¼Œä»¥ä¸¥æ ¼ã€å…¬æ­£çš„æ€åº¦è¯„ä¼°ä»£ç è´¨é‡ã€‚"
            },
            {"role": "user", "content": prompt}
        ]

        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(
                self.api_url,
                headers={
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": "glm-5",
                    "messages": messages,
                    "temperature": 0.3,  # ä½æ¸©åº¦ä»¥ä¿æŒä¸€è‡´æ€§
                    "max_tokens": 3000
                }
            )

            if response.status_code == 200:
                result = response.json()
                return result["choices"][0]["message"]["content"]
            else:
                raise Exception(f"API error: {response.status_code}")

    def _parse_audit_result(self, llm_output: str) -> Dict:
        """è§£æ LLM å®¡è®¡ç»“æœ"""
        try:
            # å°è¯•ç›´æ¥è§£æ JSON
            result = json.loads(llm_output)
            return result
        except json.JSONDecodeError:
            # å°è¯•æå– JSON ä»£ç å—
            import re
            json_match = re.search(r'```json\s*([\s\S]*?)\s*```', llm_output)
            if json_match:
                result = json.loads(json_match.group(1))
                return result
            else:
                # å¦‚æœæ— æ³•è§£æï¼Œè¿”å›ä¿å®ˆä¼°è®¡
                return {
                    "passed": False,
                    "score": 5,
                    "reasoning": "Failed to parse LLM output",
                    "issues": ["Unable to parse audit result"],
                    "improvements": ["Manual review required"]
                }

    def _rule_based_audit(self, feature: Dict, code_contents: Dict[str, str]) -> Dict:
        """
        åŸºäºè§„åˆ™çš„å®¡è®¡ï¼ˆfallbackï¼‰

        å½“ LLM ä¸å¯ç”¨æ—¶ä½¿ç”¨
        """
        issues = []
        score = 10  # åˆå§‹æ»¡åˆ†ï¼Œå‘ç°ä¸€ä¸ªé—®é¢˜æ‰£åˆ†
        passed = True

        # æ£€æŸ¥ 1: æ˜¯å¦æœ‰ä»£ç 
        if not code_contents:
            return {
                "passed": False,
                "score": 1,
                "reasoning": "No code files found",
                "issues": ["No implementation"],
                "improvements": ["Implement the feature"]
            }

        all_code = "\n".join(code_contents.values())

        # æ£€æŸ¥ 2: å ä½ç¬¦æ£€æµ‹
        placeholder_patterns = ["TODO", "PLACEHOLDER", "NOT IMPLEMENTED", "Required"]
        for pattern in placeholder_patterns:
            if pattern in all_code:
                score = min(score, 3)
                issues.append(f"Contains placeholder: {pattern}")
                passed = False
                break

        # æ£€æŸ¥ 3: ç®€å•æ‹¼æ¥æ£€æµ‹
        if "optimize" in feature["description"].lower():
            # æ£€æŸ¥æ˜¯å¦åªæœ‰å­—ç¬¦ä¸²æ‹¼æ¥
            if "result +=" in all_code or "result &" in all_code:
                if "llm." not in all_code and "api." not in all_code and "fetch" not in all_code:
                    score = min(score, 2)
                    issues.append("Simple string concatenation detected (no LLM API call)")
                    passed = False

        # æ£€æŸ¥ 4: æ–‡ä»¶å¤§å°
        total_size = sum(len(content) for content in code_contents.values())
        if total_size < 200:
            score = min(score, 4)
            issues.append(f"Code files too small ({total_size} bytes)")
            passed = False

        # æ£€æŸ¥ 5: é”™è¯¯å¤„ç†
        if "try {" not in all_code and "try:" not in all_code:
            score -= 2
            issues.append("No error handling found")

        # æ£€æŸ¥ 6: é›†æˆç‚¹éªŒè¯
        logical_reqs = feature.get("logical_requirements", {})
        if logical_reqs.get("must_call_llm") == "å¿…é¡»è°ƒç”¨ LLM API":
            if "llm." not in all_code and "api." not in all_code and "fetch" not in all_code:
                score = min(score, 2)
                issues.append("Requires LLM API call but none found")
                passed = False

        # ç¡®ä¿åˆ†æ•°åœ¨ 1-10 èŒƒå›´å†…
        score = max(1, min(10, score))

        return {
            "passed": passed and score >= 7,
            "score": score,
            "reasoning": f"Rule-based audit: {len(issues)} issues found, score={score}/10",
            "issues": issues,
            "improvements": self._generate_improvements(issues, feature)
        }

    def _generate_improvements(self, issues: List[str], feature: Dict) -> List[str]:
        """æ ¹æ®é—®é¢˜ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        improvements = []

        for issue in issues:
            if "placeholder" in issue.lower():
                improvements.append("Replace placeholder code with actual implementation")
            elif "string concatenation" in issue.lower():
                improvements.append("Implement intelligent optimization logic instead of simple string joining")
            elif "error handling" in issue.lower():
                improvements.append("Add try-catch blocks for error handling")
            elif "too small" in issue.lower():
                improvements.append("Add more implementation details and logic")
            elif "no api call" in issue.lower():
                improvements.append("Integrate with LLM API for intelligent processing")

        if not improvements:
            improvements.append("Review and enhance implementation based on requirements")

        return improvements


# å…¨å±€å‡½æ•°
async def audit_feature_quality(
    feature: Dict,
    project_path: str
) -> Dict:
    """
    ä¾¿æ·å‡½æ•°ï¼šå®¡è®¡åŠŸèƒ½è´¨é‡

    Returns:
        å®¡è®¡ç»“æœå­—å…¸
    """
    auditor = QualityAuditor()

    # æŸ¥æ‰¾ç›¸å…³ä»£ç æ–‡ä»¶
    project = Path(project_path)
    code_files = []

    # æ ¹æ®åŠŸèƒ½ç±»åˆ«æŸ¥æ‰¾ç›¸å…³æ–‡ä»¶
    category = feature.get("category", "")
    if category in ["ui", "frontend"]:
        code_files.extend([str(p.relative_to(project)) for p in project.rglob("*.tsx")])
        code_files.extend([str(p.relative_to(project)) for p in project.rglob("*.ts")])
    elif category in ["api", "backend"]:
        code_files.extend([str(p.relative_to(project)) for p in project.rglob("*.py")])

    return await auditor.audit_feature_implementation(feature, code_files, str(project_path))
